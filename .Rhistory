2 - 2
numeroA <- 7
numeroB <- 8
print(numeroA - numeroB)
print(numeroA - numeroB)
numeroA <- 9
3 - 7
numeroA <- 9
numeroB <- 4
print(numeroA - numeroB)
2*3
2**3
2**3
base <- 3
potencia <- 2
print(base**potencia)
lista <- (7,8,5,4,3,1)
lista <- c(7,8,5,4,3,1)
plot(lista)
gc()
mensaje <- "Hola Mundo"
print(mensaje)
2 + 2
numeroA <- 7
numeroB <- 8
print(numeroA + numeroB)
3 - 7
numeroA <- 9
numeroB <- 4
print(numeroA - numeroB)
2**3
base <- 3
potencia <- 2
print(base**potencia)
lista <- c(7,8,5,4,3,1)
plot(lista)
input("ajshda")
readline("aksjdhaskd")
somevar <- readline("Nombre")
Pepe
print("Hola" + somevar)
print("Hola", somevar)
somevar <- readline("Nombre")
print(somevar)
var1 <- as.integer(5) #asignando como entero
print(var1) #imprimo el entero
var2 <-as.Date(3021) #asignando como fecha
print(var2) #imprimo la fecha
var3 <- as.numeric(5)
var4 <- as.integer(5)
var5 <- as.complex(5i)
print(ls.str())
nombre <- readline(prompt="Ingrese su nombre: ")
print(paste("Mi nombre es",nombre, "y tengo",edad ,"años."))
nombre <- readline(prompt="Ingrese su nombre: ")
nombre <- readline(prompt="Ingrese su nombre: ")
print(paste("Mi nombre es",nombre, "y tengo",edad ,"años."))
nombre <- readline(prompt="Ingrese su nombre: ")
edad <- readline(prompt="Ingrese su edad: ")
print(paste("Mi nombre es",nombre, "y tengo",edad ,"años."))
vector <- c(5,6,10,8,9,8)
#Podemos hacer un ploteo simple de cualquier vector
plot(vector)
print(mean(vector)) #promedio
max (vector) #maximo
min (vector) #minimo
median (vector) #valor medio, de la secuencia ordenada
mode (vector) #valor mas comun
mode (vector) #valor mas comun
vector <- c(5,6,10,8,9,8)
mode (vector) #valor mas comun
sum(vector)
prod(vector)
10%%2
13%%2
15%%5
print(1:10)
sample(0:100, 10, replace=TRUE)
#Muestreos al azar
sample(0:100, 10, replace=TRUE)
#vista de lista
lista <- c(1:10)
print (head(lista))
print (head(lista,3))
library(DescTools)
install.packages("DescTools")
library(DescTools)
Mode(vector)
dataset <- data.frame(mes = character(),
temperatura = numeric(),
precipitaciones = numeric(),
humedad = numeric())
sort(lista,decreasing=FALSE)
morley #datos de velocidad de la luz en distintos medios
nhtemp #promedio de temperaturas por año en New Haven (vector)
mtcars #Pruebas de distintos vehiculos
nottem #temperaturas promedio en nottingham (matriz)
iris #medidas de distintas flores
?mtcars #para ver información de este dataset
datasets::airmiles
Data_autos <- mtcars
max(Data_autos$hp)
min(Data_autos$hp)
nombre <- readline("Ingrese su nombre: ")
edad <- readline("Ingrese su edad: ")
print(paste("Mi nombre es",nombre, "y tengo",edad ,"años."))
#1 - Escriba un programa de R para tomar la entrada del usuario (nombre y edad) y mostrar los valores.
nombre <- readline("Ingrese su nombre: ")
apellido <- readline("Ingrese su edad: ")
años <- readlinte("Ingrese su edad: ")
años <- readlinte("Ingrese su edad: ")
años <- readline("Ingrese su edad: ")
print(paste("Mi nombre es",apellido, ", ", nombre," ", apellido, "y tengo liencia para matar por los próximos ", años, " años."))
print(paste("Mi nombre es",apellido, ",", nombre, apellido, "y tengo liencia para matar por los próximos ", años, " años."))
#12 Del set IRIS, ordenar indicar el promedio de longitud de pétalos, solamente de la especie setosa. Luego indicar el máximo de longitud de pétalo, solo de la especie virginica. (Se puede resolver solo con lo aclarado en esta guía pero hay formas mas elegantes de hacerlo)
summart(iris)
#12 Del set IRIS, ordenar indicar el promedio de longitud de pétalos, solamente de la especie setosa. Luego indicar el máximo de longitud de pétalo, solo de la especie virginica. (Se puede resolver solo con lo aclarado en esta guía pero hay formas mas elegantes de hacerlo)
summary(iris)
getwd()
edad=c(1,2,3,5,7,9,11,13) #edad para cada ovservación
altura=c(76.1,86.4,95.2,109.1,122.0,133.7,143.7,156.4) #variable, altura de un niño
edadporaltura=data.frame(edad,altura)
print(edadporaltura) #observo el dataframe
#con el dataframe armado, trazo un plot simple
plot(edadporaltura)
#vemos que existe una relacion observable, entonces
regresion = lm(edad~altura, data=edadporaltura) #lm es la instrucción que me construye la relación. donde, pongo la variable de entrada, la variable de salida y los datos de donde se obtienen.
print(regresion) #aca vemos la variacion de la varible de salida, mas o menos 15 cm en promedio, y el punto de intercepción del 0
plot(edadporaltura)
alturanina=c(74.1,83.4,92.2,105.1,120.0,129.7,137.7,145.4)
edadalturacomp=data.frame(edad,altura,alturanina)
plot(edadalturacomp)
alturanina2=data.frame(edad,alturanina)
plot(alturanina2)
regresion2=lm(edad~alturanina, data=alturanina2)
print(regresion2)
summary(regresion)
interseccionen0 = -11.239690 #es el valor donde intersecta en la regresion
variacionporedad = 0.1527 #crecimiento promedio por edad
predecir_edad = interseccionen0 + variacionporedad * 100 #edad que tendria al alcanzar 100 cm.
print(predecir_edad)
#ahora que pasa si quiero predecir para una altura de dos metros
predecir_edad2 = interseccionen0 + variacionporedad * 200
print(predecir_edad2)
predecir_edad3 = interseccionen0 + variacionporedad * 250
print(predecir_edad3)
planetas=c("mercurio","venus","tierra","marte","jupiter","saturno","urano","neptuno","pluton")
OrdenPlaneta=c(1,2,3,4,5,6,7,8,9)
lunas=c(0,0,1,2,79,82,27,14,1) #y por consiguiente, como considero a pluton un planeta, debo considerar a caronte una luna
lunasdeSol=data.frame(planetas,OrdenPlaneta,lunas)
print(lunasdeSol)
lunasSolOrden=data.frame(OrdenPlaneta,lunas)
print(lunasSolOrden)
plot(lunasSolOrden)
regresion3=lm(OrdenPlaneta~lunas, data=lunasSolOrden)
print(regresion3)
summary(regresion3)
install.packages("ISLR")
install.packages("tidyverse")
library(tidyverse)
library(ISLR)
ordendepasajero=c(1:25) #25 pasajeros
supervivencia=c(1,1,0,0,1,1,0,1,0,0,0,1,1,0,0,0,1,0,0,1,0,1,1,0,0)
supervivencia_pasajeros=data.frame(ordendepasajero,supervivencia)
print(supervivencia_pasajeros)
modelo_logistico <- glm(supervivencia ~ ordendepasajero, data = supervivencia_pasajeros, family = "binomial")
print(modelo_logistico)
ggplot(data = supervivencia_pasajeros, aes(x = ordendepasajero, y = supervivencia)) +
geom_point(aes(color = as.factor(supervivencia)), shape = 1) +
stat_function(fun = function(x){predict(modelo_logistico,
newdata = data.frame(ordendepasajero = x),
type = "response")}) +
theme_bw() +
labs(title = "Regresión logística",
y = "Probabilidad Supervivencia") +
theme(legend.position = "none")
summary(modelo_logistico)
intercepcion_0 <- 0.32960
ordendepasajero_p <- -0.04438
prediccion_supervivencia = intercepcion_0 + ordendepasajero_p * 26
print(prediccion_supervivencia)
install.packages("caTools")
install.packages("rpart")
install.packages("rpart.plot")
install.packages("caret")
install.packages("dplyr")
install.packages("dplyr")
install.packages("dplyr")
install.packages("dplyr")
install.packages("dplyr")
install.packages("dplyr")
library(caTools)
library(rpart)
library(rpart.plot)
library(caret)
library(dplyr)
head(iris)
set.seed(42
str(iris)
particion_muestra <- sample.split(Y = iris$Species, SplitRatio = 0.75)
particion_entrenamiento <- subset(x = iris, particion_muestra == TRUE)
particion_prueba <- subset(x = iris, particion_muestra == FALSE)
modelo_arbol <- rpart(Species ~ ., data = particion_entrenamiento, method = "class")
print (modelo_arbol)
rpart.plot(modelo_arbol)
poder_prediccion <- varImp(modelo_arbol)
poder_prediccion %>%
arrange(desc(Overall))
prediccion <- predict(modelo_arbol, newdata = particion_prueba, type = "class")
print(prediccion)
plot(prediccion)
confusionMatrix(particion_prueba$Species, prediccion)
install.packages("keras")
install.packages("tensorflow")
install.packages("text2vec")
install.packages("dplyr")
install.packages("tfdatasets")
install.packages("keras")
install.packages("tensorflow")
install.packages("text2vec")
install.packages("dplyr")
install.packages("tfdatasets")
reticulate::py_install('transformers', pip = TRUE)
transformer = reticulate::import('transformers')
reticulate::py_install('transformers', pip = TRUE)
summary(movie_review)
library(text2vec)
data("movie_review")
df = movie_review %>% rename(target = sentiment, comment_text = review) %>%
sample_n(2000) %>%
data.table::as.data.table()
ai_m = list(
c('TFGPT2Model',       'GPT2Tokenizer',       'gpt2'),
c('TFRobertaModel',    'RobertaTokenizer',    'roberta-base'),
c('TFElectraModel',    'ElectraTokenizer',    'google/electra-small-generator')
)
max_len = 50L
epochs = 2
batch_size = 10
gather_history = list()
for (i in 1:length(ai_m)) {
tokenizer = glue::glue("transformer${ai_m[[i]][2]}$from_pretrained('{ai_m[[i]][3]}',
do_lower_case=TRUE)") %>%
rlang::parse_expr() %>% eval()
model_ = glue::glue("transformer${ai_m[[i]][1]}$from_pretrained('{ai_m[[i]][3]}')") %>%
rlang::parse_expr() %>% eval()
# inputs
text = list()
# outputs
label = list()
data_prep = function(data) {
for (i in 1:nrow(data)) {
txt = tokenizer$encode(data[['comment_text']][i],max_length = max_len,
truncation=T) %>%
t() %>%
as.matrix() %>% list()
lbl = data[['target']][i] %>% t()
text = text %>% append(txt)
label = label %>% append(lbl)
}
list(do.call(plyr::rbind.fill.matrix,text), do.call(plyr::rbind.fill.matrix,label))
}
train_ = data_prep(train)
test_ = data_prep(test)
tf_train = tensor_slices_dataset(list(train_[[1]],train_[[2]])) %>%
dataset_batch(batch_size = batch_size, drop_remainder = TRUE) %>%
dataset_shuffle(128) %>% dataset_repeat(epochs) %>%
dataset_prefetch(tf$data$experimental$AUTOTUNE)
tf_test = tensor_slices_dataset(list(test_[[1]],test_[[2]])) %>%
dataset_batch(batch_size = batch_size)
input = layer_input(shape=c(max_len), dtype='int32')
hidden_mean = tf$reduce_mean(model_(input)[[1]], axis=1L) %>%
layer_dense(64,activation = 'relu')
output = hidden_mean %>% layer_dense(units=1, activation='sigmoid')
model = keras_model(inputs=input, outputs = output)
model %>% compile(optimizer= tf$keras$optimizers$Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0),
loss = tf$losses$BinaryCrossentropy(from_logits=F),
metrics = tf$metrics$AUC())
print(glue::glue('{ai_m[[i]][1]}'))
library(caTools)
library(rpart)
library(rpart.plot)
library(caret)
library(dplyr)
library(caTools)
library(rpart)
library(rpart.plot)
library(caret)
library(dplyr)
head(iris)
set.seed(42)
str(iris)
particion_muestra <- sample.split(Y = iris$Species, SplitRatio = 0.75)
particion_entrenamiento <- subset(x = iris, particion_muestra == TRUE)
particion_prueba <- subset(x = iris, particion_muestra == FALSE)
modelo_arbol <- rpart(Species ~ ., data = particion_entrenamiento, method = "class")
print (modelo_arbol)
rpart.plot(modelo_arbol)
poder_prediccion <- varImp(modelo_arbol)
poder_prediccion %>% arrange(desc(Overall))
prediccion <- predict(modelo_arbol, newdata = particion_prueba, type = "class")
print(prediccion)
plot(prediccion)
confusionMatrix(particion_prueba$Species, prediccion)
library(rpart)
control <- rpart.control(minsplit = 20, minbucket = 7, maxdepth=30)
fit <- rpart(Species ~ ., data=iris, method="class", control=control)
modelo_arbol_corregido <- rpart(Species ~ ., data = particion_entrenamiento, method="class", control=control)
rpart.plot(modelo_arbol_corregido)
library("ggplot2")
diam = ggplot2::diamonds
particion_muestra <- sample.split(Y = diam$cut, SplitRatio = 0.75)
particion_train <- subset(x = diam, particion_muestra == TRUE)
particion_test <- subset(x = diam, particion_muestra == FALSE)
arbol_diamonds <- rpart(cut ~ ., data = particion_train, method = "class")
rpart.plot(arbol_diamonds)
poder_prediccion <- varImp(arbol_diamonds)
poder_prediccion %>% arrange(desc(Overall))
prediccion_diam <- predict(arbol_diamonds, newdata = particion_test, type = "class")
plot(prediccion_diam)
confusionMatrix(particion_test$cut, prediccion_diam)
library(rpart)
control <- rpart.control(minsplit = 20, minbucket = 7, maxdepth=30)
fit <- rpart(cut ~ ., data=diam, method="class", control=control)
modelo_arbol_corregido <- rpart(cut ~ ., data = particion_train, method="class", control=control)
rpart.plot(modelo_arbol_corregido)
stress = read.csv("StressLevelDataset.csv")
stress = read.csv("StressLevelDataset.csv")
vars = c("anxiety_level", 'self_esteem','sleep_quality', 'blood_pressure','living_conditions','stress_level' )
stress = stress[vars]
stress$stress_level = as.factor(stress$stress_level)
particion_muestra <- sample.split(Y = stress$stress_level, SplitRatio = 0.75)
particion_train <- subset(x = stress, particion_muestra == TRUE)
particion_test <- subset(x = stress, particion_muestra == FALSE)
arbol_stress<- rpart(stress_level ~ ., data = particion_train, method = "class")
rpart.plot(arbol_stress)
poder_prediccion <- varImp(arbol_stress)
poder_prediccion %>% arrange(desc(Overall))
prediccion_stress <- predict(arbol_stress, newdata = particion_test, type = "class")
plot(prediccion_stress)
confusionMatrix(particion_test$stress_level, prediccion_stress)
accuracy_stress = confusionMatrix(particion_test$stress_level, prediccion_stress)$overall[1]
print(accuracy_stress)
splits = seq(1,100, by=10)
minbuckets = seq(1,100, by=10)
maxdepths  = seq(1,30, by=5)
library(rpart)
generar_arbol = function(stress, particion_train, particion_test, n_split, n_minbucket, n_maxdepth) {
#Obs. uso el mismo particion_train y particion_test aplciado sobre el DF stress
control <- rpart.control(minsplit = n_split, minbucket = n_minbucket, maxdepth = n_maxdepth) #parametros
arbol_nuevo <- rpart(stress_level ~ ., data = particion_train, method="class", control=control)
prediccion_stress <- predict(arbol_nuevo, newdata = particion_test, type = "class")
accuracy_stress = confusionMatrix(particion_test$stress_level, prediccion_stress)$overall[1]
return (c(n_split, n_minbucket, n_maxdepth, accuracy_stress))
}
columns = c("n_split", "n_minbucket", "n_maxdepth", "accuracy")
hiperparametros = data.frame(matrix(nrow = 0, ncol = length(columns)))
colnames(hiperparametros) = columns
for (i in splits){
for (j in minbuckets) {
for (k in maxdepths) {
resultado = generar_arbol(stress, particion_train, particion_test, i,j,k)
print(paste(i,j,k))
hiperparametros[nrow(hiperparametros)+1,] = resultado
}}}
a = hiperparametros[hiperparametros$accuracy == max(hiperparametros$accuracy) ,]
print(a)
plot(a)
install.packages("keras")
install.packages("tensorflow")
install.packages("text2vec")
install.packages("dplyr")
install.packages("tfdatasets")
reticulate::py_install('transformers', pip = TRUE)
summary(movie_review)
str(movie_review)
?movie_review
library(text2vec)
data("movie_review")
df = movie_review %>% rename(target = sentiment, comment_text = review) %>%
sample_n(2000) %>%
data.table::as.data.table()
idx_train = sample.int(nrow(df)*0.8)
train = df[idx_train,]
test = df[!idx_train,]
ai_m = list(
c('TFGPT2Model',       'GPT2Tokenizer',       'gpt2'),
c('TFRobertaModel',    'RobertaTokenizer',    'roberta-base'),
c('TFElectraModel',    'ElectraTokenizer',    'google/electra-small-generator')
)
max_len = 50L
epochs = 2
batch_size = 10
gather_history = list()
for (i in 1:length(ai_m)) {
# tokenizador
tokenizer = glue::glue("transformer${ai_m[[i]][2]}$from_pretrained('{ai_m[[i]][3]}',
do_lower_case=TRUE)") %>%
rlang::parse_expr() %>% eval()
# modelo
model_ = glue::glue("transformer${ai_m[[i]][1]}$from_pretrained('{ai_m[[i]][3]}')") %>%
rlang::parse_expr() %>% eval()
# inputs
text = list()
# outputs
label = list()
data_prep = function(data) {
for (i in 1:nrow(data)) {
txt = tokenizer$encode(data[['comment_text']][i],max_length = max_len,
truncation=T) %>%
t() %>%
as.matrix() %>% list()
lbl = data[['target']][i] %>% t()
text = text %>% append(txt)
label = label %>% append(lbl)
}
list(do.call(plyr::rbind.fill.matrix,text), do.call(plyr::rbind.fill.matrix,label))
}
train_ = data_prep(train)
test_ = data_prep(test)
# corte del dataset
tf_train = tensor_slices_dataset(list(train_[[1]],train_[[2]])) %>%
dataset_batch(batch_size = batch_size, drop_remainder = TRUE) %>%
dataset_shuffle(128) %>% dataset_repeat(epochs) %>%
dataset_prefetch(tf$data$experimental$AUTOTUNE)
tf_test = tensor_slices_dataset(list(test_[[1]],test_[[2]])) %>%
dataset_batch(batch_size = batch_size)
# creamos capa de inserción
input = layer_input(shape=c(max_len), dtype='int32')
hidden_mean = tf$reduce_mean(model_(input)[[1]], axis=1L) %>%
layer_dense(64,activation = 'relu')
# creamos una capa de salida para clasificación binaria
output = hidden_mean %>% layer_dense(units=1, activation='sigmoid')
model = keras_model(inputs=input, outputs = output)
# compilamos con AUC score
model %>% compile(optimizer= tf$keras$optimizers$Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0),
loss = tf$losses$BinaryCrossentropy(from_logits=F),
metrics = tf$metrics$AUC())
print(glue::glue('{ai_m[[i]][1]}'))
# entrenamos el modelo
history = model %>% keras::fit(tf_train, epochs=epochs, #paso a paso por epoca; steps_per_epoch=len/batch_size,
validation_data=tf_test)
gather_history[[i]]<- history
names(gather_history)[i] = ai_m[[i]][1]
}
summary(movie_review)
str(movie_review)
?movie_review
#este dataset es binario, donde 1 identifica reviews positivas y 0 a las negativas.
#preparamos los datos:
library(text2vec)
data("movie_review")
df = movie_review %>% rename(target = sentiment, comment_text = review) %>%
sample_n(2000) %>%
data.table::as.data.table()
library(text2vec)
data("movie_review")
df = movie_review %>% rename(target = sentiment, comment_text = review) %>%
sample_n(2000) %>%
data.table::as.data.table()
history = model %>% keras::fit(tf_train, epochs=epochs, #paso a paso por epoca; steps_per_epoch=len/batch_size,
validation_data=tf_test)
train_ = data_prep(train)
head(iris)
set.seed(42) #generador de numeros al azar
#iniciamos la separacion de la muestra
str(iris) #del set de Iris
#tomamos el particionamos en 75% y 25%
particion_muestra <- sample.split(Y = iris$Species, SplitRatio = 0.75)
library(caTools)
library(rpart)
library(rpart.plot)
library(caret)
library(dplyr)
head(iris)
set.seed(42) #generador de numeros al azar
#iniciamos la separacion de la muestra
str(iris)
particion_muestra <- sample.split(Y = iris$Species, SplitRatio = 0.75)
#creamos el set de entrenamiento tomando los verdaderos - 75%
particion_entrenamiento <- subset(x = iris, particion_muestra == TRUE)
#creamos el set de prueba tomando los falsos - 25%
particion_prueba <- subset(x = iris, particion_muestra == FALSE)
#usando la libreria rpart, creamos el modelo. Ponemos la variable objetivo a la izquierda.
modelo_arbol <- rpart(Species ~ ., data = particion_entrenamiento, method = "class")
print (modelo_arbol)
rpart.plot(modelo_arbol)
poder_prediccion <- varImp(modelo_arbol)
poder_prediccion %>% arrange(desc(Overall))
print(prediccion)
plot(prediccion)
confusionMatrix(particion_prueba$Species, prediccion)
confusionMatrix(particion_prueba$Species, prediccion)
#Ahora, como ajustar hiperparametros...facil:
library(rpart)
control <- rpart.control(minsplit = 20, minbucket = 7, maxdepth=30)
fit <- rpart(Species ~ ., data=iris, method="class", control=control)
#y se genera el arbol como parametros ajustados
modelo_arbol_corregido <- rpart(Species ~ ., data = particion_entrenamiento, method="class", control=control)
#ploteamos corregido
rpart.plot(modelo_arbol_corregido)
library("ggplot2")
# Ejercicio 1: Usando uno de los datasets integrados crear un arbol de clasificación.
# Recordar verificar que los datos sean adecuados y que la muestra sea representativa.
diam = ggplot2::diamonds #la idea es clasificar el tipo de diamante (cut)
#Particiono: 75% train, 25% test
particion_muestra <- sample.split(Y = diam$cut, SplitRatio = 0.75)
particion_train <- subset(x = diam, particion_muestra == TRUE) #Set de training 75% de los datos
particion_test <- subset(x = diam, particion_muestra == FALSE)#Set de testing 25% de los datos
arbol_diamonds <- rpart(cut ~ ., data = particion_train, method = "class") #method = class XQ es clasificacion, sino para una regresion seria lm, poisson, exp, etc.
#Grafico el arbol de clasificacion
rpart.plot(arbol_diamonds)
